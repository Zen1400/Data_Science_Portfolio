{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2785320c",
   "metadata": {},
   "source": [
    "## Before I get into $PCA$ I would like to brush up on some basic Linear Algebra concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c926501",
   "metadata": {},
   "source": [
    "### Dot Product\n",
    "\n",
    "* Dot Product is a directional multiplication, it describes the amount by which one vector goes in the direction of the another.\n",
    "\n",
    "\n",
    "$$\n",
    "\\vec{a}.\\vec{b} = |a|.|b|.cos(\\theta)\n",
    "$$\n",
    "***\n",
    " \n",
    "### Projection\n",
    "\n",
    "* You can think of the projection of $\\vec{a}$ on $\\vec{b}$ as the amount of information from $\\vec{a}$ that is contained in $\\vec{b}$, the projection is maximum when the two vectors are parallel and zero when they are orthogonal\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "***\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"dot.png\" width=600 height=400 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abe1ea1",
   "metadata": {},
   "source": [
    "### Covariance Matrix :\n",
    "\n",
    "- Symmetric matrix (m*m) because Covariance is commutative : Cov(a,b) = Cov(b,a)  where m is the number of dimensions \n",
    "\n",
    "\n",
    "- Covariance Matrix :     $ {C(x,y)} = \\left[ \\begin{array}{c} Cov(x,x) & Cov(x,y) \\\\ Cov(y,x) & Cov(y,y) \\end{array} \\right]$\n",
    "\n",
    "\n",
    "- $Cov(x,y) = \\sum_{i=1}^{n} \\frac{(x-\\bar {x})(y- \\bar {y} )}{n}$ \n",
    "\n",
    "\n",
    "- If the Covariance is positive means that the variables increase or decrease together, otherwise if it's negative means that one variable decreases while the other increases \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf01af",
   "metadata": {},
   "source": [
    "## Eigenvectors & Eigenvalues\n",
    "- Eigenvectors are vectors that don't change their span line after a linear transformation(multiplying by a matrix), only they get longer or shorter by an amount which is the Eigenvalue of a given vector.\n",
    "\n",
    "\n",
    "- $ A\\vec{v} = \\lambda \\vec{v} \\space\\space\\space\\space ======> (A - \\lambda I) \\vec{v} = \\vec{0} $   \n",
    "\n",
    "\n",
    "- For non zero vector, the only way to satisfy the previous equation is that the $det(A -\\lambda I) = 0$ meaning that the transformation associated with A collapses the space into a lower dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975288b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92fbe12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee93ce4a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104a449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9586d0da",
   "metadata": {},
   "source": [
    "## What is PCA ?\n",
    "\n",
    "- $Principal Component Analysis (PCA)$ is an unsupervised dimensionality reduction technique. The goal of PCA is to project the dataset onto a lower-dimensional space while preserving as much of the variance of the dataset as possible.\n",
    "\n",
    "\n",
    "- $Principal Components$ : Uncorrelated(perpendicular) Linear Combinations of the initial variables, PCA tries to put maximum info in the first components, ex : for 5 dimensions data set we would have 5 principal components and the first ones would have most of the information. B doing it this way we can discard the last PCs so we reduce dimensionality without losing too much info.\n",
    "<img src=\"pc.png\" height=500 width=400 />\n",
    "\n",
    "- $PCA$ can be performed in 6 steps:\n",
    "\n",
    "#### 1) Subtract the mean of each variable :\n",
    "- This step is so important because PCA tries to maximize the variance, so if the varaiance of one data point is so much bigger than others the PCA will try to project as much as possible in the direction of this data point resulting in a bias \n",
    "\n",
    "#### 2) Calculate the Covariance Matrix :\n",
    "- To understand the relationship between variables how they vary from the mean whith respect to each other.\n",
    "- Ex : When data points are highly correlated that means that there's a redundant information \n",
    "\n",
    "#### 3) Compute the Eigenvalues and Eigenvectors :\n",
    "- \n",
    "\n",
    "4) Sort Eigenvectors by corresponding Eigenvalues in descending order and select a subset from the rearranged Eigenvalue matrix\n",
    "\n",
    "5) Recast data along the principal components\n",
    "_____________________________________________________________________________________________________________________________\n",
    "_____________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec94c8",
   "metadata": {},
   "source": [
    "- $PCA$ combines features to produce new features that are uncolrrelated and orderd in terms of importance called $Principal$ \n",
    " $Components$\n",
    " \n",
    " How we can arrange data points on a line in a way that perserves as much information as possible ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c66d6f",
   "metadata": {},
   "source": [
    "- The first component of PCA is a unit vector that tries to maximize the information preserved meaning it tries to maximize the projection of the data points on it which is the the square of the dot product between data points and the unit vector.\n",
    "\n",
    "\n",
    "- To maximize the dot product of data points is an optimization problem :        $Max   \\sum (x_i^T.\\vec{u})^2$ Where $x_i$ is a data point and $\\vec{u}$ is the first component which is a unit vector\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557656d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
