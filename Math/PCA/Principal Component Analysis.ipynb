{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2785320c",
   "metadata": {},
   "source": [
    "## Before I get into $PCA$ I would like to brush up on some basic Linear Algebra concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c926501",
   "metadata": {},
   "source": [
    "### Dot Product\n",
    "\n",
    "* Dot Product is a directional multiplication, it describes the amount by which one vector goes in the direction of the another.\n",
    "\n",
    "\n",
    "$$\n",
    "\\vec{a}.\\vec{b} = |a|.|b|.cos(\\theta)\n",
    "$$\n",
    "***\n",
    " \n",
    "### Projection\n",
    "\n",
    "* You can think of the projection of $\\vec{a}$ on $\\vec{b}$ as the amount of information from $\\vec{a}$ that is contained in $\\vec{b}$, the projection is maximum when the two vectors are parallel and zero when they are orthogonal\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "***\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"dot.png\" width=600 height=400 />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe1c4e",
   "metadata": {},
   "source": [
    "### Covariance Matrix :\n",
    "\n",
    "- Symmetric matrix (m*m) because Covariance is commutative : Cov(a,b) = Cov(b,a)  where m is the number of dimensions \n",
    "\n",
    "\n",
    "- Covariance Matrix :     $ {C(x,y)} = \\left[ \\begin{array}{c} Cov(x,x) & Cov(x,y) \\\\ Cov(y,x) & Cov(y,y) \\end{array} \\right]$\n",
    "\n",
    "\n",
    "- $Cov(x,y) = \\sum_{i=1}^{n} \\frac{(x_i-\\bar {x})(y_i- \\bar {y} )}{n}$ \n",
    "\n",
    "\n",
    "- If the Covariance is positive means that the variables increase or decrease together, otherwise if it's negative means that one variable decreases while the other increases \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b9a44",
   "metadata": {},
   "source": [
    "## Eigenvectors & Eigenvalues\n",
    "- Eigenvectors are vectors that don't change their span line after a linear transformation(multiplying by a matrix), only they get longer or shorter by an amount which is the Eigenvalue of a given vector.\n",
    "\n",
    "\n",
    "- $ A\\vec{v} = \\lambda \\vec{v} \\space\\space\\space\\space ======> (A - \\lambda I) \\vec{v} = \\vec{0} $   \n",
    "\n",
    "\n",
    "- For non zero vector, the only way to satisfy the previous equation is that the $det(A -\\lambda I) = 0$ meaning that the transformation associated with A collapses the space into a lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc835ea",
   "metadata": {},
   "source": [
    "### Why Eigenvectors point in the direction of the maximum variance ?\n",
    "\n",
    "- We know that we want to maximize the dot product(projection of data points on our unit vector)   $\\begin {align*}Max \\sum (x_i^T \\vec{u})^2 \\\\ = Max \\sum \\vec{u} x x \\vec{u}  \\\\ Let A = \\sum x^T x \\\\ = Max \\sum \\vec{u^T} A \\vec{u} \\end {align*}    \n",
    "\\\\ $\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0c626",
   "metadata": {},
   "source": [
    "- We can reformulate a constrained optimization problem : \" Maximize f(x) under the constraint g(x) = c \", as an unconstrained optimization using  $Lagrange Multipliers$ :\n",
    "\n",
    " $$\\mathcal{L}(x, \\lambda) = f(x) - \\lambda (g(x) - c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f14cc",
   "metadata": {},
   "source": [
    "-  So in PCA or constraint is that the vector is a unit vector : $u^T \\vec{u} = 1 $     \n",
    "Note : Wen want $\\vec{u}$ to be a unit vector because of the maximization equation, otherwise we could maximize the length of the vector to maximize the function as well.\n",
    "\n",
    "\n",
    "- So our $Lagrange Function$ will be : \n",
    "$$ \\mathcal{L}(\\vec{u}, \\lambda) = u^TA\\vec{u} - \\lambda(u^T \\vec{u} - 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96d8fc",
   "metadata": {},
   "source": [
    "- We know that the gradient at the $Maxima$ equals zero, accordingly : \n",
    "\n",
    "$$\\frac {\\partial \\mathcal{L}(\\vec{u}, \\lambda)}{\\partial \\vec{u}} = 2 A \\vec{u} - 2 \\lambda \\vec{u} = 0 =====> A \\vec{u} = \\lambda \\vec{u}$$\n",
    "\n",
    "\n",
    "- So we get that the unit vector that maximizes the variance is the $Eigenvector$ of the covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e8cea",
   "metadata": {},
   "source": [
    "- So the amount of information after projection on the Eigenvector $\\vec{u}$ is given by the corresponding Eigenvalue $\\lambda$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0f44a",
   "metadata": {},
   "source": [
    "### Why we substract the mean in PCA ?\n",
    "\n",
    "\n",
    "<img src=\"m.png\" height=600 width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9586d0da",
   "metadata": {},
   "source": [
    "## What is PCA ?\n",
    "\n",
    "- $Principal Component Analysis (PCA)$ is an unsupervised dimensionality reduction technique. The goal of PCA is to project the dataset onto a lower-dimensional space while preserving as much of the variance of the dataset as possible.\n",
    "\n",
    "\n",
    "- $PCA$ combines features to produce new features that are uncolrrelated and orderd in terms of importance called $Principal$ \n",
    " $Components$\n",
    " \n",
    " How we can arrange data points on a line in a way that perserves as much information as possible ?\n",
    "\n",
    "\n",
    "- The first component of PCA is a unit vector that tries to maximize the information preserved meaning it tries to maximize the projection of the data points on it which is the the square of the dot product between data points and the unit vector.\n",
    "\n",
    "\n",
    "- To maximize the dot product of data points is an optimization problem :        $Max   \\sum (x_i^T.\\vec{u})^2$ Where $x_i$ is a data point and $\\vec{u}$ is the first component which is a unit vector\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "- $Principal Components$ : Uncorrelated(perpendicular) Linear Combinations of the initial variables, PCA tries to put maximum info in the first components, ex : for 5 dimensions data set we would have 5 principal components and the first ones would have most of the information. B doing it this way we can discard the last PCs so we reduce dimensionality without losing too much info.\n",
    "<img src=\"pc.png\" height=500 width=400 />\n",
    "\n",
    "### $PCA$ can be performed in 6 steps:\n",
    "\n",
    "#### 1) Subtract the mean of each variable :\n",
    "- This step is so important because PCA tries to maximize the variance, so if the varaiance of one data point is so much bigger than others the PCA will try to project as much as possible in the direction of this data point resulting in a bias \n",
    "\n",
    "#### 2) Calculate the Covariance Matrix :\n",
    "- To understand the relationship between variables how they vary from the mean whith respect to each other.\n",
    "- Ex : When data points are highly correlated that means that there's a redundant information \n",
    "\n",
    "#### 3) Compute the Eigenvalues and Eigenvectors :\n",
    "\n",
    "\n",
    "#### 4) Sort Eigenvectors by corresponding Eigenvalues in descending order and select a subset from the rearranged Eigenvalue matrix\n",
    "- In this step we select the feature vectors and we reduce dimensionality while taking into consideration multiple factors like how much information loss we can accept etc...\n",
    "\n",
    "#### 5) Recast data along the principal components\n",
    "- After choosing the feature vectors we multiply the transpose of the standardized dataset by the transpose of the matrix of feature vectors. \n",
    "_____________________________________________________________________________________________________________________________\n",
    "_____________________________________________________________________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
